"""
Backup Service - 3-2-1 Backup Strategy Implementation

3 copies, 2 media types, 1 offsite + immutable

Backup Types:
1. Continuous: Append-only audit log (every transaction)
2. Snapshot: Daily full backup (MongoDB dump)
3. Incremental: Hourly delta backups

Storage Layers:
1. Local: Fast recovery (optional venue NAS)
2. Cloud: S3/equivalent (primary offsite)
3. Immutable: WORM storage (compliance)
"""

from datetime import datetime, timezone, timedelta
from pathlib import Path
import subprocess
import json
import os
import hashlib
import asyncio
from typing import Optional, Dict, Any

class BackupService:
    def __init__(self, db, config: Optional[Dict[str, Any]] = None):
        self.db = db
        self.config = config or {}
        
        # Backup paths â€” use /tmp on cloud (always writable), configurable otherwise
        _default_backup = os.environ.get('BACKUP_DIR', '/tmp/backups/local')
        self.local_backup_dir = Path(self.config.get('local_backup_dir', _default_backup))
        self.cloud_backup_enabled = self.config.get('cloud_backup_enabled', False)
        self.s3_bucket = self.config.get('s3_bucket', None)
        
        # NAS (optional venue-local)
        self.nas_enabled = self.config.get('nas_enabled', False)
        self.nas_path = Path(self.config.get('nas_path', '/mnt/venue_nas')) if self.nas_enabled else None
        
        # Retention
        self.retention_days = self.config.get('retention_days', 90)
        
        # Ensure directories exist (graceful â€” don't crash app on permission errors)
        try:
            self.local_backup_dir.mkdir(parents=True, exist_ok=True)
        except PermissionError:
            # Fallback to /tmp if default path is not writable
            self.local_backup_dir = Path('/tmp/backups/local')
            self.local_backup_dir.mkdir(parents=True, exist_ok=True)
        if self.nas_enabled and self.nas_path:
            try:
                self.nas_path.mkdir(parents=True, exist_ok=True)
            except PermissionError:
                self.nas_enabled = False

    # ============= SNAPSHOT BACKUP =============

    async def create_snapshot(self, venue_id: str = None) -> Dict[str, Any]:
        """Create full MongoDB snapshot backup"""
        timestamp = datetime.now(timezone.utc)
        snapshot_id = f"snapshot_{timestamp.strftime('%Y%m%d_%H%M%S')}"
        
        if venue_id:
            snapshot_id = f"{snapshot_id}_{venue_id}"

        snapshot_dir = self.local_backup_dir / snapshot_id
        snapshot_dir.mkdir(exist_ok=True)

        try:
            # MongoDB dump using mongodump
            mongo_url = os.environ.get('MONGO_URL', 'mongodb://localhost:27017')
            db_name = os.environ.get('DB_NAME', 'restin')
            
            dump_path = snapshot_dir / 'mongodb_dump'
            
            cmd = [
                'mongodump',
                '--uri', mongo_url,
                '--db', db_name,
                '--out', str(dump_path)
            ]
            
            if venue_id:
                # Venue-scoped backup
                cmd.extend(['--query', json.dumps({"venue_id": venue_id})])

            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"mongodump failed: {result.stderr}")

            # Create metadata
            metadata = {
                "snapshot_id": snapshot_id,
                "timestamp": timestamp.isoformat(),
                "venue_id": venue_id,
                "type": "full_snapshot",
                "status": "completed",
                "size_bytes": self.get_directory_size(dump_path)
            }

            # Calculate checksum (integrity verification)
            metadata["checksum"] = self.calculate_directory_checksum(dump_path)

            # Save metadata
            with open(snapshot_dir / 'metadata.json', 'w') as f:
                json.dump(metadata, f, indent=2)

            # Log to database
            await self.db.backups.insert_one({
                **metadata,
                "local_path": str(snapshot_dir),
                "created_at": timestamp.isoformat()
            })

            # Copy to NAS if enabled
            if self.nas_enabled and self.nas_path:
                await self.copy_to_nas(snapshot_dir, snapshot_id)

            # Upload to cloud if enabled
            if self.cloud_backup_enabled and self.s3_bucket:
                await self.upload_to_s3(snapshot_dir, snapshot_id)

            print(f"âœ… Snapshot backup created: {snapshot_id}")
            return metadata

        except Exception as e:
            print(f"âŒ Snapshot backup failed: {e}")
            
            await self.db.backups.insert_one({
                "snapshot_id": snapshot_id,
                "timestamp": timestamp.isoformat(),
                "venue_id": venue_id,
                "type": "full_snapshot",
                "status": "failed",
                "error": str(e),
                "created_at": timestamp.isoformat()
            })
            
            raise

    # ============= CONTINUOUS BACKUP (Audit Log) =============

    async def append_audit_entry(self, entry: Dict[str, Any]):
        """Append-only audit log (immutable, never deleted)"""
        audit_file = self.local_backup_dir / 'audit_log.jsonl'
        
        # Add hash chain for immutability
        entry_with_hash = {
            **entry,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "prev_hash": await self.get_last_audit_hash(),
            "sequence": await self.get_next_audit_sequence()
        }
        
        # Calculate this entry's hash
        entry_with_hash["hash"] = self.calculate_entry_hash(entry_with_hash)

        # Append to file (append-only)
        with open(audit_file, 'a') as f:
            f.write(json.dumps(entry_with_hash) + '\n')

        return entry_with_hash

    async def get_last_audit_hash(self) -> Optional[str]:
        """Get hash of last audit entry for chain verification"""
        audit_file = self.local_backup_dir / 'audit_log.jsonl'
        
        if not audit_file.exists():
            return None

        try:
            with open(audit_file, 'r') as f:
                lines = f.readlines()
                if lines:
                    last_entry = json.loads(lines[-1])
                    return last_entry.get('hash')
        except:
            return None

        return None

    async def get_next_audit_sequence(self) -> int:
        """Get next sequence number for audit log"""
        audit_file = self.local_backup_dir / 'audit_log.jsonl'
        
        if not audit_file.exists():
            return 1

        try:
            with open(audit_file, 'r') as f:
                lines = f.readlines()
                if lines:
                    last_entry = json.loads(lines[-1])
                    return last_entry.get('sequence', 0) + 1
        except:
            return 1

        return 1

    def calculate_entry_hash(self, entry: Dict[str, Any]) -> str:
        """Calculate SHA-256 hash of audit entry"""
        # Exclude hash field itself
        entry_copy = {k: v for k, v in entry.items() if k != 'hash'}
        entry_str = json.dumps(entry_copy, sort_keys=True)
        return hashlib.sha256(entry_str.encode()).hexdigest()

    # ============= RESTORE & VERIFICATION =============

    async def verify_backup(self, snapshot_id: str) -> Dict[str, Any]:
        """Verify backup integrity via checksum"""
        snapshot_dir = self.local_backup_dir / snapshot_id
        metadata_file = snapshot_dir / 'metadata.json'

        if not metadata_file.exists():
            return {"valid": False, "error": "Metadata not found"}

        with open(metadata_file, 'r') as f:
            metadata = json.load(f)

        # Recalculate checksum
        dump_path = snapshot_dir / 'mongodb_dump'
        current_checksum = self.calculate_directory_checksum(dump_path)

        if current_checksum != metadata.get('checksum'):
            return {
                "valid": False,
                "error": "Checksum mismatch - backup corrupted",
                "expected": metadata.get('checksum'),
                "actual": current_checksum
            }

        return {"valid": True, "snapshot_id": snapshot_id}

    async def test_restore(self, snapshot_id: str) -> Dict[str, Any]:
        """Test restore to temporary database (weekly verification)"""
        # This would restore to a test database and verify data integrity
        # For safety, not implementing full restore here
        
        verification = await self.verify_backup(snapshot_id)
        
        if not verification.get('valid'):
            return verification

        # Log successful verification
        await self.db.backup_verifications.insert_one({
            "snapshot_id": snapshot_id,
            "verified_at": datetime.now(timezone.utc).isoformat(),
            "status": "passed",
            "checksum_valid": True
        })

        return {"success": True, "verified": True}

    # ============= HELPERS =============

    def get_directory_size(self, path: Path) -> int:
        """Calculate total size of directory"""
        total = 0
        for entry in path.rglob('*'):
            if entry.is_file():
                total += entry.stat().st_size
        return total

    def calculate_directory_checksum(self, path: Path) -> str:
        """Calculate combined checksum of all files in directory"""
        hasher = hashlib.sha256()
        
        for entry in sorted(path.rglob('*')):
            if entry.is_file():
                with open(entry, 'rb') as f:
                    while chunk := f.read(8192):
                        hasher.update(chunk)
        
        return hasher.hexdigest()

    async def copy_to_nas(self, snapshot_dir: Path, snapshot_id: str):
        """Copy backup to venue NAS (if available)"""
        if not self.nas_enabled or not self.nas_path:
            return

        nas_dest = self.nas_path / snapshot_id
        
        try:
            subprocess.run(['cp', '-r', str(snapshot_dir), str(nas_dest)], check=True)
            print(f"ğŸ“€ Backup copied to NAS: {nas_dest}")
        except Exception as e:
            print(f"âš ï¸ NAS copy failed (non-critical): {e}")

    async def upload_to_s3(self, snapshot_dir: Path, snapshot_id: str):
        """Upload backup to S3 (offsite)"""
        # Placeholder - would use boto3 or similar
        print(f"â˜ï¸ S3 upload would happen here: {snapshot_id}")
        # Implementation: aws s3 sync <snapshot_dir> s3://<bucket>/<snapshot_id>

    async def cleanup_old_backups(self):
        """Delete backups older than retention period"""
        cutoff = datetime.now(timezone.utc) - timedelta(days=self.retention_days)
        cutoff_str = cutoff.isoformat()

        result = await self.db.backups.delete_many({
            "created_at": {"$lt": cutoff_str}
        })

        print(f"ğŸ—‘ï¸ Cleaned up {result.deleted_count} old backup records")
        return result.deleted_count
